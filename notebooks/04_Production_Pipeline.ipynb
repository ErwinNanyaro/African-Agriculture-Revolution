# Google Colab Notebook for Production Pipeline
# Save as: C:\African-Agriculture-Revolution\notebooks\04_Production_Pipeline.ipynb

"""
üåç AFRICAN AGRICULTURE DATA SCIENCE REVOLUTION
üè≠ Production Pipeline & Automation
‚ö° From Analysis to Automated Insights
"""

# ============================================
# 1. PRODUCTION ENVIRONMENT SETUP
# ============================================

# Install production packages
!pip install pandas numpy scikit-learn xgboost -q
!pip install fastapi uvicorn pydantic -q
!pip install joblib mlflow -q
!pip install python-dotenv -q

# Import production libraries
import pandas as pd
import numpy as np
import pickle
import joblib
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

print("‚úÖ Production environment ready!")

# ============================================
# 2. LOAD PRODUCTION DATA
# ============================================

# Clone repository
import os
if not os.path.exists('African-Agriculture-Revolution'):
    !git clone https://github.com/ErwinNanyaro/African-Agriculture-Revolution.git
    %cd African-Agriculture-Revolution
else:
    %cd African-Agriculture-Revolution

print("üìÅ Loading data for production pipeline...")

# Load both datasets
try:
    strawberry_df = pd.read_csv('data/raw/strawberry_farmers_Morogoro.csv')
    arusha_df = pd.read_csv('data/raw/both_crops_farmers_Arusha.csv')
    
    print(f"‚úÖ Data loaded:")
    print(f"   üçì Strawberry: {strawberry_df.shape[0]} records")
    print(f"   üåΩ Arusha: {arusha_df.shape[0]} records")
    
except Exception as e:
    print(f"‚ùå Error loading data: {e}")
    # Create sample data for demonstration
    print("‚ö†Ô∏è Creating sample data for demonstration...")
    strawberry_df = pd.DataFrame({
        'age_group': ['30_40', '41_50', 'above_50'] * 10,
        'education': ['primary', 'secondary', 'no_formal_education'] * 10,
        'farm_size': ['1_3_acres', '4_6_acres', '1_3_acres'] * 10,
        'yield_change': ['increased', 'decreased', 'remained_the_same'] * 10
    })
    arusha_df = pd.DataFrame({
        'Age': [25, 35, 45, 55, 65] * 4,
        'Education Level': ['Primary', 'Secondary', 'Higher Education'] * 7,
        'farm_size': [2, 3, 4, 5, 6] * 4,
        'adaptation_strategy': ['Crop Rotation,Irrigation', 'New crop variety', 'Mixed farming'] * 7
    })

# ============================================
# 3. PRODUCTION DATA PIPELINE
# ============================================

class ProductionDataPipeline:
    """Production-ready data processing pipeline"""
    
    def __init__(self):
        self.pipeline_steps = []
        self.data_transforms = {}
        
    def process_strawberry_data(self, df):
        """Process strawberry data for production"""
        print("üîß Processing strawberry data...")
        
        df_proc = df.copy()
        
        # Step 1: Standardize column names
        df_proc.columns = [col.lower().replace(' ', '_').replace('-', '_') for col in df_proc.columns]
        
        # Step 2: Create yield target
        if 'yield_change' in df_proc.columns:
            yield_mapping = {
                'increased': 1,
                'remained_the_same': 0,
                'decreased': -1
            }
            df_proc['yield_target'] = df_proc['yield_change'].map(yield_mapping)
        
        # Step 3: Encode categorical variables
        categorical_cols = df_proc.select_dtypes(include=['object']).columns
        
        for col in categorical_cols:
            if df_proc[col].nunique() < 20:  # Avoid high cardinality
                df_proc[f'{col}_encoded'] = pd.factorize(df_proc[col])[0]
        
        self.pipeline_steps.append('strawberry_processed')
        self.data_transforms['strawberry'] = {
            'original_shape': df.shape,
            'processed_shape': df_proc.shape,
            'categorical_encoded': len(categorical_cols)
        }
        
        return df_proc
    
    def process_arusha_data(self, df):
        """Process Arusha data for production"""
        print("üîß Processing Arusha data...")
        
        df_proc = df.copy()
        
        # Step 1: Standardize column names
        df_proc.columns = [str(col).lower().replace(' ', '_').replace('-', '_') for col in df_proc.columns]
        
        # Step 2: Extract adaptation strategies
        strategy_cols = [col for col in df_proc.columns if 'strategy' in col or 'adapt' in col]
        if strategy_cols:
            strategy_col = strategy_cols[0]
            # Count strategies
            df_proc['strategy_count'] = df_proc[strategy_col].apply(
                lambda x: len(str(x).split(',')) if pd.notna(x) else 0
            )
        
        # Step 3: Create adaptation score
        adaptation_indicators = [
            'extension_service', 'training', 'technology', 
            'financial_resource', 'community_participation'
        ]
        
        df_proc['adaptation_score'] = 0
        for indicator in adaptation_indicators:
            indicator_cols = [col for col in df_proc.columns if indicator in col]
            if indicator_cols:
                indicator_col = indicator_cols[0]
                # Simple scoring: 1 if contains "yes", 0 otherwise
                df_proc['adaptation_score'] += df_proc[indicator_col].apply(
                    lambda x: 1 if 'yes' in str(x).lower() else 0
                )
        
        self.pipeline_steps.append('arusha_processed')
        self.data_transforms['arusha'] = {
            'original_shape': df.shape,
            'processed_shape': df_proc.shape,
            'adaptation_score_range': (df_proc['adaptation_score'].min(), df_proc['adaptation_score'].max())
        }
        
        return df_proc
    
    def merge_datasets(self, strawberry_df, arusha_df):
        """Merge datasets for unified analysis"""
        print("üîÑ Merging datasets...")
        
        # Find common columns
        common_cols = list(set(strawberry_df.columns) & set(arusha_df.columns))
        
        # Add source identifiers
        strawberry_df['data_source'] = 'strawberry'
        strawberry_df['region'] = 'Morogoro'
        
        arusha_df['data_source'] = 'arusha'
        arusha_df['region'] = 'Arusha'
        
        # Merge on common columns + identifiers
        merged_cols = common_cols + ['data_source', 'region']
        merged_df = pd.concat([
            strawberry_df[[col for col in merged_cols if col in strawberry_df.columns]],
            arusha_df[[col for col in merged_cols if col in arusha_df.columns]]
        ], ignore_index=True)
        
        self.pipeline_steps.append('datasets_merged')
        self.data_transforms['merged'] = {
            'total_records': len(merged_df),
            'strawberry_records': len(strawberry_df),
            'arusha_records': len(arusha_df),
            'common_features': len(common_cols)
        }
        
        return merged_df
    
    def get_pipeline_report(self):
        """Generate pipeline execution report"""
        report = "="*60 + "\n"
        report += "PRODUCTION PIPELINE EXECUTION REPORT\n"
        report += "="*60 + "\n\n"
        
        report += f"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        report += f"Steps executed: {len(self.pipeline_steps)}\n\n"
        
        for step in self.pipeline_steps:
            report += f"‚úÖ {step}\n"
        
        report += "\nüìä DATA TRANSFORMATIONS:\n"
        for dataset, transforms in self.data_transforms.items():
            report += f"\n{dataset.upper()}:\n"
            for key, value in transforms.items():
                report += f"  {key}: {value}\n"
        
        return report

# Execute pipeline
print("\nüöÄ EXECUTING PRODUCTION PIPELINE...")
pipeline = ProductionDataPipeline()

# Process datasets
strawberry_processed = pipeline.process_strawberry_data(strawberry_df)
arusha_processed = pipeline.process_arusha_data(arusha_df)

# Merge datasets
merged_data = pipeline.merge_datasets(strawberry_processed, arusha_processed)

# Generate report
print("\n" + pipeline.get_pipeline_report())

# ============================================
# 4. PRODUCTION MODEL TRAINING
# ============================================

print("\nü§ñ TRAINING PRODUCTION MODELS")

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import joblib

class ProductionModelTrainer:
    """Production model training pipeline"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.model_metrics = {}
    
    def train_yield_prediction_model(self, df):
        """Train model to predict yield changes"""
        print("üìà Training yield prediction model...")
        
        # Prepare data
        if 'yield_target' in df.columns:
            # Select features
            feature_cols = [col for col in df.columns 
                           if col not in ['yield_target', 'yield_change', 'data_source', 'region'] 
                           and ('encoded' in col or df[col].dtype in ['int64', 'float64'])]
            
            X = df[feature_cols].fillna(0)
            y = df['yield_target']
            
            # Remove constant columns
            X = X.loc[:, X.nunique() > 1]
            
            if len(X.columns) > 0 and len(y.unique()) > 1:
                # Split data
                X_train, X_test, y_train, y_test = train_test_split(
                    X, y, test_size=0.2, random_state=42
                )
                
                # Scale features
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)
                
                # Train model
                model = RandomForestClassifier(n_estimators=100, random_state=42)
                model.fit(X_train_scaled, y_train)
                
                # Evaluate
                train_score = model.score(X_train_scaled, y_train)
                test_score = model.score(X_test_scaled, y_test)
                
                # Save artifacts
                self.models['yield_predictor'] = model
                self.scalers['yield_scaler'] = scaler
                self.model_metrics['yield_predictor'] = {
                    'train_accuracy': train_score,
                    'test_accuracy': test_score,
                    'features_used': len(X.columns),
                    'classes': list(y.unique())
                }
                
                print(f"   ‚úÖ Model trained: {test_score:.3f} accuracy")
                return model, scaler
        
        print("   ‚ö†Ô∏è Insufficient data for yield prediction")
        return None, None
    
    def train_adaptation_scoring_model(self, df):
        """Train model to score adaptation effectiveness"""
        print("üå± Training adaptation scoring model...")
        
        # Prepare data
        if 'adaptation_score' in df.columns:
            # Select features
            feature_cols = [col for col in df.columns 
                           if col not in ['adaptation_score', 'data_source', 'region'] 
                           and ('encoded' in col or df[col].dtype in ['int64', 'float64'])]
            
            X = df[feature_cols].fillna(0)
            y = df['adaptation_score']
            
            # Remove constant columns
            X = X.loc[:, X.nunique() > 1]
            
            if len(X.columns) > 0:
                # Split data
                X_train, X_test, y_train, y_test = train_test_split(
                    X, y, test_size=0.2, random_state=42
                )
                
                # Scale features
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)
                
                # Train model
                model = RandomForestRegressor(n_estimators=100, random_state=42)
                model.fit(X_train_scaled, y_train)
                
                # Evaluate
                train_score = model.score(X_train_scaled, y_train)
                test_score = model.score(X_test_scaled, y_test)
                
                # Save artifacts
                self.models['adaptation_scorer'] = model
                self.scalers['adaptation_scaler'] = scaler
                self.model_metrics['adaptation_scorer'] = {
                    'train_r2': train_score,
                    'test_r2': test_score,
                    'features_used': len(X.columns),
                    'target_range': (y.min(), y.max())
                }
                
                print(f"   ‚úÖ Model trained: R¬≤ = {test_score:.3f}")
                return model, scaler
        
        print("   ‚ö†Ô∏è Insufficient data for adaptation scoring")
        return None, None
    
    def save_models(self, directory='production_models'):
        """Save all models and scalers"""
        print(f"\nüíæ Saving models to {directory}...")
        
        os.makedirs(directory, exist_ok=True)
        
        # Save models
        for name, model in self.models.items():
            filename = f"{directory}/{name}.pkl"
            joblib.dump(model, filename)
            print(f"   ‚úÖ Saved {name}: {filename}")
        
        # Save scalers
        for name, scaler in self.scalers.items():
            filename = f"{directory}/{name}.pkl"
            joblib.dump(scaler, filename)
            print(f"   ‚úÖ Saved {name}: {filename}")
        
        # Save metadata
        metadata = {
            'timestamp': datetime.now().isoformat(),
            'models_trained': list(self.models.keys()),
            'metrics': self.model_metrics
        }
        
        import json
        with open(f'{directory}/metadata.json', 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"   ‚úÖ Saved metadata: {directory}/metadata.json")
        
        return directory
    
    def get_model_report(self):
        """Generate model training report"""
        report = "="*60 + "\n"
        report += "MODEL TRAINING REPORT\n"
        report += "="*60 + "\n\n"
        
        report += f"Models trained: {len(self.models)}\n"
        report += f"Scalers created: {len(self.scalers)}\n\n"
        
        for model_name, metrics in self.model_metrics.items():
            report += f"{model_name.upper()}:\n"
            for key, value in metrics.items():
                report += f"  {key}: {value}\n"
            report += "\n"
        
        return report

# Train models
print("\nüî® BUILDING PRODUCTION MODELS...")
trainer = ProductionModelTrainer()

# Train on strawberry data (for yield prediction)
yield_model, yield_scaler = trainer.train_yield_prediction_model(strawberry_processed)

# Train on Arusha data (for adaptation scoring)
adaptation_model, adaptation_scaler = trainer.train_adaptation_scoring_model(arusha_processed)

# Save models
models_dir = trainer.save_models()

# Generate report
print("\n" + trainer.get_model_report())

# ============================================
# 5. PRODUCTION API DEVELOPMENT
# ============================================

print("\nüåê CREATING PRODUCTION API")

# Create API code
api_code = '''
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import pandas as pd
import numpy as np
import joblib
from typing import List, Optional
import os

# Load production models
MODELS_DIR = "production_models"

try:
    yield_model = joblib.load(f"{MODELS_DIR}/yield_predictor.pkl")
    yield_scaler = joblib.load(f"{MODELS_DIR}/yield_scaler.pkl")
    adaptation_model = joblib.load(f"{MODELS_DIR}/adaptation_scorer.pkl")
    adaptation_scaler = joblib.load(f"{MODELS_DIR}/adaptation_scaler.pkl")
    print("‚úÖ Production models loaded")
except:
    print("‚ö†Ô∏è Models not found, running in demo mode")
    yield_model = None
    adaptation_model = None

# Define API
app = FastAPI(title="African Agriculture API",
              description="Production API for farmer predictions",
              version="1.0.0")

# Data models
class FarmerFeatures(BaseModel):
    age_group: str
    education: str
    farm_size: str
    group_member: Optional[str] = "no"
    training_received: Optional[str] = "no"
    
class PredictionResponse(BaseModel):
    prediction: float
    confidence: float
    recommendation: str

# API endpoints
@app.get("/")
def read_root():
    return {"message": "African Agriculture Production API", 
            "status": "active",
            "models_loaded": yield_model is not None}

@app.post("/predict/yield", response_model=PredictionResponse)
def predict_yield(features: FarmerFeatures):
    """Predict yield change for a farmer"""
    if yield_model is None:
        return PredictionResponse(
            prediction=0.5,
            confidence=0.8,
            recommendation="Model not loaded, using default prediction"
        )
    
    # Convert features to DataFrame
    df = pd.DataFrame([features.dict()])
    
    # Preprocess features (simplified for demo)
    # In production, implement full preprocessing pipeline
    
    # Make prediction
    try:
        prediction = 0.7  # Placeholder
        confidence = 0.85
        
        # Generate recommendation
        if prediction > 0.6:
            recommendation = "High yield potential detected. Continue current practices."
        elif prediction > 0.3:
            recommendation = "Moderate yield potential. Consider joining farmer group."
        else:
            recommendation = "Low yield potential detected. Recommend training and resource support."
        
        return PredictionResponse(
            prediction=prediction,
            confidence=confidence,
            recommendation=recommendation
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
def health_check():
    return {"status": "healthy", "timestamp": pd.Timestamp.now().isoformat()}

# To run: uvicorn api:app --reload
'''

# Save API code
api_filename = "production_api.py"
with open(api_filename, 'w') as f:
    f.write(api_code)

print(f"‚úÖ API code saved to: {api_filename}")

# Create requirements file for API
api_requirements = '''
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0
pandas==2.1.3
numpy==1.24.3
scikit-learn==1.3.2
joblib==1.3.2
python-dotenv==1.0.0
'''

with open("api_requirements.txt", 'w') as f:
    f.write(api_requirements)

print("‚úÖ API requirements saved to: api_requirements.txt")

# ============================================
# 6. AUTOMATION SCRIPTS
# ============================================

print("\nü§ñ CREATING AUTOMATION SCRIPTS")

# Create data pipeline automation script
pipeline_script = '''
#!/usr/bin/env python
"""
üè≠ African Agriculture Data Pipeline
Automated data processing and model retraining
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
import sys
import logging
from pathlib import Path

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('data_pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class AutomatedDataPipeline:
    """Automated pipeline for regular data processing"""
    
    def __init__(self, data_dir='data/raw', output_dir='data/processed'):
        self.data_dir = Path(data_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def run_daily_pipeline(self):
        """Run daily data processing"""
        logger.info("Starting daily data pipeline...")
        
        # 1. Check for new data files
        new_files = self._check_new_files()
        
        if not new_files:
            logger.info("No new files found")
            return False
        
        # 2. Process each new file
        for filepath in new_files:
            try:
                self._process_file(filepath)
                logger.info(f"Processed {filepath.name}")
            except Exception as e:
                logger.error(f"Error processing {filepath}: {e}")
        
        # 3. Update master dataset
        self._update_master_dataset()
        
        # 4. Check if model retraining is needed
        self._check_model_retraining()
        
        logger.info("Daily pipeline completed successfully")
        return True
    
    def _check_new_files(self):
        """Check for new data files"""
        processed_files = set()
        if (self.output_dir / 'processed_files.txt').exists():
            with open(self.output_dir / 'processed_files.txt', 'r') as f:
                processed_files = set(f.read().splitlines())
        
        new_files = []
        for filepath in self.data_dir.glob('*.csv'):
            if str(filepath) not in processed_files:
                new_files.append(filepath)
        
        return new_files
    
    def _process_file(self, filepath):
        """Process a single data file"""
        # Read file
        df = pd.read_csv(filepath)
        
        # Basic processing
        df.columns = [col.lower().replace(' ', '_') for col in df.columns]
        df['processed_date'] = datetime.now().date()
        
        # Save processed file
        output_path = self.output_dir / f"processed_{filepath.name}"
        df.to_csv(output_path, index=False)
        
        # Record as processed
        with open(self.output_dir / 'processed_files.txt', 'a') as f:
            f.write(f"{filepath}\\n")
    
    def _update_master_dataset(self):
        """Update master dataset with new data"""
        processed_files = list(self.output_dir.glob('processed_*.csv'))
        
        if processed_files:
            # Combine all processed files
            all_data = []
            for filepath in processed_files:
                df = pd.read_csv(filepath)
                all_data.append(df)
            
            master_df = pd.concat(all_data, ignore_index=True)
            master_path = self.output_dir / 'master_dataset.csv'
            master_df.to_csv(master_path, index=False)
            
            logger.info(f"Master dataset updated: {len(master_df)} records")
    
    def _check_model_retraining(self):
        """Check if models need retraining"""
        master_path = self.output_dir / 'master_dataset.csv'
        
        if master_path.exists():
            master_df = pd.read_csv(master_path)
            
            # Simple check: retrain if we have 100+ new records
            new_records = master_df[
                pd.to_datetime(master_df['processed_date']) > 
                datetime.now() - timedelta(days=7)
            ]
            
            if len(new_records) >= 100:
                logger.info(f"Found {len(new_records)} new records, triggering model retraining")
                # In production, this would trigger a model retraining job
                return True
        
        return False

# Main execution
if __name__ == "__main__":
    pipeline = AutomatedDataPipeline()
    
    try:
        success = pipeline.run_daily_pipeline()
        if success:
            print("‚úÖ Pipeline executed successfully")
            sys.exit(0)
        else:
            print("‚ÑπÔ∏è No new data to process")
            sys.exit(0)
    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        sys.exit(1)
'''

with open("automated_pipeline.py", 'w') as f:
    f.write(pipeline_script)

print("‚úÖ Automation script saved to: automated_pipeline.py")

# Create scheduling script
scheduling_script = '''
#!/usr/bin/env python
"""
‚è∞ African Agriculture Scheduler
Schedule automated pipeline runs
"""

import schedule
import time
from datetime import datetime
import subprocess
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def run_data_pipeline():
    """Run the data pipeline"""
    logger.info(f"Running data pipeline at {datetime.now()}")
    
    try:
        result = subprocess.run(
            ['python', 'automated_pipeline.py'],
            capture_output=True,
            text=True,
            timeout=300  # 5 minute timeout
        )
        
        if result.returncode == 0:
            logger.info("Pipeline completed successfully")
            logger.info(result.stdout)
        else:
            logger.error(f"Pipeline failed: {result.stderr}")
    
    except subprocess.TimeoutExpired:
        logger.error("Pipeline timed out after 5 minutes")
    except Exception as e:
        logger.error(f"Error running pipeline: {e}")

def run_weekly_report():
    """Generate weekly report"""
    logger.info(f"Generating weekly report at {datetime.now()}")
    # Implement weekly reporting
    pass

def run_monthly_model_retraining():
    """Retrain models monthly"""
    logger.info(f"Starting monthly model retraining at {datetime.now()}")
    # Implement model retraining
    pass

# Schedule jobs
schedule.every().day.at("02:00").do(run_data_pipeline)  # Daily at 2 AM
schedule.every().monday.at("03:00").do(run_weekly_report)  # Weekly on Monday at 3 AM
schedule.every().first.monday.at("04:00").do(run_monthly_model_retraining)  # First Monday of month

if __name__ == "__main__":
    logger.info("Starting African Agriculture Scheduler...")
    logger.info("Scheduled jobs:")
    logger.info("  - Daily data pipeline: 02:00 every day")
    logger.info("  - Weekly report: 03:00 every Monday")
    logger.info("  - Monthly retraining: 04:00 first Monday of month")
    
    # Run immediately on start
    run_data_pipeline()
    
    # Keep scheduler running
    while True:
        schedule.run_pending()
        time.sleep(60)  # Check every minute
'''

with open("scheduler.py", 'w') as f:
    f.write(scheduling_script)

print("‚úÖ Scheduler script saved to: scheduler.py")

# ============================================
# 7. DEPLOYMENT CONFIGURATION
# ============================================

print("\nüöÄ CREATING DEPLOYMENT CONFIGURATION")

# Dockerfile for production
dockerfile = '''
# African Agriculture Revolution - Production Dockerfile
FROM python:3.9-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements
COPY requirements.txt .
COPY api_requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt
RUN pip install --no-cache-dir -r api_requirements.txt

# Copy application code
COPY src/ ./src/
COPY production_models/ ./production_models/
COPY production_api.py .
COPY automated_pipeline.py .

# Create data directories
RUN mkdir -p data/raw data/processed results

# Expose API port
EXPOSE 8000

# Health check
HEALTHCHECK CMD curl --fail http://localhost:8000/health || exit 1

# Run API
CMD ["uvicorn", "production_api:app", "--host", "0.0.0.0", "--port", "8000"]
'''

with open("Dockerfile", 'w') as f:
    f.write(dockerfile)

print("‚úÖ Dockerfile created")

# Docker Compose for full stack
docker_compose = '''
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./data:/app/data
      - ./results:/app/results
    environment:
      - ENVIRONMENT=production
      - LOG_LEVEL=INFO
    restart: unless-stopped
  
  pipeline:
    build: .
    command: python automated_pipeline.py
    volumes:
      - ./data:/app/data
      - ./results:/app/results
    environment:
      - ENVIRONMENT=production
    restart: on-failure
    depends_on:
      - api
  
  scheduler:
    build: .
    command: python scheduler.py
    volumes:
      - ./data:/app/data
      - ./results:/app/results
    environment:
      - ENVIRONMENT=production
    restart: unless-stopped
    depends_on:
      - api

volumes:
  data:
  results:
'''

with open("docker-compose.yml", 'w') as f:
    f.write(docker_compose)

print("‚úÖ Docker Compose file created")

# ============================================
# 8. PRODUCTION READINESS CHECKLIST
# ============================================

print("\nüìã PRODUCTION READINESS CHECKLIST")

checklist = [
    ("‚úÖ", "Data processing pipeline implemented"),
    ("‚úÖ", "Machine learning models trained"),
    ("‚úÖ", "Models serialized and saved"),
    ("‚úÖ", "Production API created"),
    ("‚úÖ", "Automation scripts ready"),
    ("‚úÖ", "Scheduling system implemented"),
    ("‚úÖ", "Docker configuration created"),
    ("üî≤", "Unit tests written"),
    ("üî≤", "Integration tests implemented"),
    ("üî≤", "Monitoring setup"),
    ("üî≤", "Alerting configured"),
    ("üî≤", "Backup strategy defined"),
    ("üî≤", "Security audit completed"),
    ("üî≤", "Performance testing done"),
    ("üî≤", "Documentation completed"),
    ("üî≤", "Disaster recovery plan")
]

print("\nüìä PRODUCTION READINESS STATUS:")
for status, item in checklist:
    print(f"   {status} {item}")

# Calculate readiness percentage
completed = sum(1 for status, _ in checklist if status == "‚úÖ")
total = len(checklist)
readiness_pct = (completed / total) * 100

print(f"\nüéØ Production Readiness: {readiness_pct:.0f}%")
print(f"   ({completed}/{total} items completed)")

# ============================================
# 9. DEPLOYMENT INSTRUCTIONS
# ============================================

print("\nüìö DEPLOYMENT INSTRUCTIONS")

instructions = """
üöÄ HOW TO DEPLOY TO PRODUCTION:

1. LOCAL TESTING:
   ```bash
   # Test the API locally
   uvicorn production_api:app --reload
   
   # Test the pipeline
   python automated_pipeline.py
   
   # Test the scheduler
   python scheduler.py